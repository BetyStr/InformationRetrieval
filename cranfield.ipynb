{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfQo2UKpZ9jT"
   },
   "source": [
    "# First Term Project: Cranfield Collection\n",
    "“The Cranfield collection [...] was the pioneering test collection in allowing CRANFIELD precise quantitative measures of information retrieval effectiveness [...]. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs.” [1, Section 8.2]\n",
    "\n",
    "Your tasks, reviewed by your colleagues and the course instructors, are the following:\n",
    "\n",
    "1.   *Implement an unsupervised ranked retrieval system*, [1, Chapter 6] which will produce a list of documents from the Cranfield collection in a descending order of relevance to a query from the Cranfield collection. You MUST NOT use relevance judgements from the Cranfield collection in your information retrieval system. Relevance judgements MUST only be used for the evaluation of your information retrieval system.\n",
    "\n",
    "2.   *Document your code* in accordance with [PEP 257](https://www.python.org/dev/peps/pep-0257/), ideally using [the NumPy style guide](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard) as seen in the code from exercises.  \n",
    "     *Stick to a consistent coding style* in accordance with [PEP 8](https://www.python.org/dev/peps/pep-0008/).\n",
    "\n",
    "3.   *Reach at least 22% mean average precision* [1, Section 8.4] with your system on the Cranfield collection. You MUST record your score either in [the public leaderboard](https://docs.google.com/spreadsheets/d/e/2PACX-1vT0FoFzCptIYKDsbcv8LebhZDe_20GFeBAPmS-VyImlWbqET0T7I2iWy59p9SHbUe3LX1yJMhALPcCY/pubhtml) or in this Jupyter notebook. You are encouraged to use techniques for tokenization, [1, Section 2.2] document representation [1, Section 6.4], tolerant retrieval [1, Chapter 3], relevance feedback and query expansion, [1, Chapter 9] and others discussed in the course.\n",
    "\n",
    "4.   _[Upload an .ipynb file](https://is.muni.cz/help/komunikace/spravcesouboru#k_ss_1) with this Jupyter notebook to the homework vault in IS MU._ You MAY also include a brief description of your information retrieval system and a link to an external service such as [Google Colaboratory](https://colab.research.google.com/), [DeepNote](https://deepnote.com/), or [JupyterHub](https://iirhub.cloud.e-infra.cz/).\n",
    "\n",
    "[1] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. [*Introduction to information retrieval*](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Cambridge university press, 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmpR8qpTZwyP"
   },
   "source": [
    "## Loading the Cranfield collection\n",
    "\n",
    "First, we will install [our library](https://gitlab.fi.muni.cz/xstefan3/pv211-utils) and load the Cranfield collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 28353,
     "status": "ok",
     "timestamp": 1677167216350,
     "user": {
      "displayName": "Alzbeta Strompova",
      "userId": "06106631639492580939"
     },
     "user_tz": -60
    },
    "id": "inUAfc6TQMVJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install git+https://github.com/MIR-MU/pv211-utils.git\n",
    "! pip install gensim==3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y845E0ePZqeH"
   },
   "source": [
    "### Loading the documents\n",
    "\n",
    "Next, we will define a class named `Document` that will represent a preprocessed document from the Cranfield collection. Tokenization and preprocessing of the `title` and `body` attributes of the individual documents as well as the creative use of the `authors`, `bibliography`, and `title` attributes is left to your imagination and craftsmanship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1677167217827,
     "user": {
      "displayName": "Alzbeta Strompova",
      "userId": "06106631639492580939"
     },
     "user_tz": -60
    },
    "id": "fyAqWIQyINng"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.cranfield.entities import CranfieldDocumentBase\n",
    "\n",
    "class Document(CranfieldDocumentBase):\n",
    "    \"\"\"\n",
    "    A preprocessed Cranfield collection document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_id : str\n",
    "        A unique identifier of the document.\n",
    "    authors : list of str\n",
    "        A unique identifiers of the authors of the document.\n",
    "    bibliography : str\n",
    "        The bibliographical entry for the document.\n",
    "    title : str\n",
    "        The title of the document.\n",
    "    body : str\n",
    "        The abstract of the document.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, document_id: str, authors: str, bibliography: str, title: str, body: str):\n",
    "        super().__init__(document_id, authors, bibliography, title, body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwnMPmFjK_FQ"
   },
   "source": [
    "We will load documents into the `documents` [ordered dictionary](https://docs.python.org/3.8/library/collections.html#collections.OrderedDict). Each document is an instance of the `Document` class that we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677167217827,
     "user": {
      "displayName": "Alzbeta Strompova",
      "userId": "06106631639492580939"
     },
     "user_tz": -60
    },
    "id": "HfRrW7O6U5wb"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.cranfield.loader import load_documents\n",
    "\n",
    "documents = load_documents(Document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwYwHs-MpD1_"
   },
   "source": [
    "### Loading the queries\n",
    "Next, we will define a class named `Query` that will represent a preprocessed query from the Cranfield collection. Tokenization and preprocessing of the `body` attribute of the individual queries is left to your craftsmanship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1677167218129,
     "user": {
      "displayName": "Alzbeta Strompova",
      "userId": "06106631639492580939"
     },
     "user_tz": -60
    },
    "id": "oaCFkMFdKjST"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.cranfield.entities import CranfieldQueryBase\n",
    "\n",
    "class Query(CranfieldQueryBase):\n",
    "    \"\"\"\n",
    "    A preprocessed Cranfield collection query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_id : int\n",
    "        A unique identifier of the query.\n",
    "    body : str\n",
    "        The text of the query.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, query_id: int, body: str):\n",
    "        super().__init__(query_id, body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-aAREbRMXeJ"
   },
   "source": [
    "We will load queries into the `queries` [ordered dictionary](https://docs.python.org/3.8/library/collections.html#collections.OrderedDict). Each query is an instance of the `Query` class that we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1677167218129,
     "user": {
      "displayName": "Alzbeta Strompova",
      "userId": "06106631639492580939"
     },
     "user_tz": -60
    },
    "id": "8qcyQUNRqRTr"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.cranfield.loader import load_queries\n",
    "\n",
    "queries = load_queries(Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJM9TfbEPCZV"
   },
   "source": [
    "## Implementation of your information retrieval system\n",
    "Next, we will define a class named `IRSystem` that will represent your information retrieval system. Your class must define a method name `search` that takes a query and returns documents in descending order of relevance to the query.\n",
    "\n",
    "The example implementation returns documents in decreasing order of the bag-of-words cosine similarity between the document and the query. The example implementation returns documents in decreasing order of the cosine similarity between the document and the query. You can use the example implementation as a basis of your system, or you can replace it with your own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwrCzoaZhWi4"
   },
   "source": [
    "## Evaluation\n",
    "Finally, we will evaluate your information retrieval system using [the Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) (MAP) evaluation measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from pv211_utils.cranfield.irsystem import CranfieldIRSystemBase\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "\n",
    "def preprocess_text(txt: str, stemmer: PorterStemmer, stop_words: set[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function that preprocess text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : str\n",
    "        text to be preprocessed\n",
    "    stemmer : PorterStemmer\n",
    "        stemmer that will be used\n",
    "    stop_words : set[str]\n",
    "        set of word that will be removed like \"and\", \"is\", ...\n",
    "    \"\"\"\n",
    "    data = simple_preprocess(txt)\n",
    "    return [stemmer.stem(i) for i in data if i not in stop_words]\n",
    "\n",
    "\n",
    "def add_weights_to_document(doc: Document) -> str:\n",
    "    \"\"\"\n",
    "    Function that add weights to document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : Document\n",
    "        text to be preprocessed\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "        a weighted sum of parts of document\n",
    "\n",
    "    \"\"\"\n",
    "    return doc.body + 2 * doc.title + 4 * doc.authors + 2 * doc.bibliography\n",
    "\n",
    "\n",
    "class IRSystem(CranfieldIRSystemBase):\n",
    "    \"\"\"\n",
    "    My model consist from two parts\n",
    "     - The first part is sentence transformer from https://www.sbert.net/\n",
    "     - The second part is TF_IDF from gensim from https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # load stopwords and stemmer\n",
    "        nltk.download('stopwords')\n",
    "        self._stemmer = PorterStemmer()\n",
    "        self._stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # the first part -> sentence transformer\n",
    "        # load model and transform documents to vectors\n",
    "        self._model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self._semantic_document_vectors = []\n",
    "        for doc in list(documents.values()):\n",
    "            self._semantic_document_vectors.append(self._model.encode(doc.body, convert_to_tensor=True))\n",
    "\n",
    "        # the second part -> TF-IDF model from gensim\n",
    "        # preprocess documents\n",
    "        document_bodies = (preprocess_text(add_weights_to_document(doc), self._stemmer, self._stop_words) for doc in documents.values())\n",
    "        document_bodies = tqdm(document_bodies, desc='Building the dictionary', total=len(documents))\n",
    "        # create model from preprocess documents\n",
    "        dictionary = Dictionary(document_bodies)\n",
    "        tfidf_model = TfidfModel(dictionary=dictionary, smartirs='lnc')\n",
    "        # transform preprocessed documents to vectors\n",
    "        document_vectors = [tfidf_model[dictionary.doc2bow(preprocess_text(add_weights_to_document(doc), self._stemmer, self._stop_words))] for doc in documents.values()]\n",
    "        document_vectors = tqdm(document_vectors, desc='Building the index', total=len(documents))\n",
    "        # create SparseMatrixSimilarity and save index and index_to_document to later use\n",
    "        index = SparseMatrixSimilarity(document_vectors, num_docs=len(documents), num_terms=len(dictionary))\n",
    "        index_to_document = dict(enumerate(documents.values()))\n",
    "\n",
    "        self.dictionary = dictionary\n",
    "        self.index = index\n",
    "        self.index_to_document = index_to_document\n",
    "\n",
    "\n",
    "    def search(self, query: Query) -> Iterable[Document]:\n",
    "        # dict that contains weighted sum of both similarities\n",
    "        similarities_dict = {}\n",
    "\n",
    "        # second part\n",
    "        # preprocess query and transform it to vector and create model\n",
    "        tfidf_query = TfidfModel(dictionary=self.dictionary, smartirs='atc')[self.dictionary.doc2bow(preprocess_text(query.body, self._stemmer, self._stop_words))]\n",
    "        # list of tuples document number and his similarity to the query\n",
    "        tfidf_similarities = enumerate(self.index[tfidf_query])\n",
    "        # add similarities to similarities_dict\n",
    "        for document_number, similarity in tfidf_similarities:\n",
    "            similarities_dict[document_number] = 1.3 * similarity  # normalization\n",
    "            # number 1.3 is not random, is rounded maximum of tf-idf similarities divided by maximum of sentence transformer similarities\n",
    "\n",
    "        # first part\n",
    "        # transform query to vector using sentence transformer\n",
    "        query_transformer = self._model.encode(query.body)\n",
    "        # add similarities to similarities_dict\n",
    "        for i in range(len(self._semantic_document_vectors)):\n",
    "            similarities_dict[i] += util.cos_sim(query_transformer, self._semantic_document_vectors[i]).tolist()[0][0]\n",
    "\n",
    "        # sort similarities and return the best ones\n",
    "        result = [(key, value) for key, value in similarities_dict.items()]\n",
    "        result = sorted(result, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        for document_number, _ in result:\n",
    "            document = self.index_to_document[document_number]\n",
    "            yield document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing your system ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AlžbetaStrompová\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Building the dictionary: 100%|██████████| 1400/1400 [00:02<00:00, 566.25it/s]\n",
      "Building the index: 100%|██████████| 1400/1400 [00:00<00:00, 34147.63it/s]\n",
      "Querying your system: 100%|██████████| 225/225 [00:21<00:00, 10.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Your system achieved **49.10% MAP score**."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Congratulations, you passed the **22%** minimum! 🥳"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Your result has been submitted to [the leaderboard](https://docs.google.com/spreadsheets/d/e/2PACX-1vSXuOTclZfHWYxh2rf7hfMeLvcCuE5UsJu7BzteyunhPw3z4YNZjCovjmMB6SnDdgjGyenOgdochaEq/pubhtml)! 🏆"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pv211_utils.cranfield.loader import load_judgements\n",
    "from pv211_utils.cranfield.leaderboard import CranfieldLeaderboard\n",
    "from pv211_utils.cranfield.eval import CranfieldEvaluation\n",
    "submit_result = True\n",
    "author_name = 'Strompová, Alžbeta'\n",
    "\n",
    "print('Initializing your system ...')\n",
    "system = IRSystem()\n",
    "evaluation = CranfieldEvaluation(system, load_judgements(queries, documents), CranfieldLeaderboard(), author_name)\n",
    "evaluation.evaluate(tqdm(queries.values(), desc='Querying your system'), submit_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "y845E0ePZqeH",
    "CwYwHs-MpD1_"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/MIR-MU/pv211-utils/blob/main/notebooks/cranfield.ipynb",
     "timestamp": 1677002233533
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
